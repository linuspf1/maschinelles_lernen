{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendung von maschinellem Lernen auf den KHK_Klassifikation.csv Datensatz\n",
    "\n",
    "## Praktische Demonstration f√ºr verschiedene machine Learning Modelle\n",
    "\n",
    "### Tim Bleicher, Linus Pfeifer\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert die Anwendung von verschiedenen Machine Learning Modellen auf den KHK_Klassifikation.csv Datensatz. \n",
    "\n",
    "# Inhaltsverzeichnis\n",
    "\n",
    "## 1. Einbindung der Daten\n",
    "- **1.1 Explorative Analyse der Daten**\n",
    "\n",
    "## 2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten\n",
    "- **Funktionsweise von PCA**\n",
    "- **L√§sst sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen?**\n",
    "\n",
    "## 3. Anwendung verschiedener Klassifikationsverfahren\n",
    "- **Definition und Datenvorbereitung**\n",
    "- **3.1 Logistische Regression**\n",
    "  - 3.1.1 Modell definieren und trainieren\n",
    "  - 3.1.2 Modell testen\n",
    "- **3.2 Entscheidungsb√§ume**\n",
    "  - 3.2.1 Klassische Entscheidungsb√§ume\n",
    "  - 3.2.2 Bagging in Form von Random Forest\n",
    "  - 3.2.3 Boosting in Form von AdaBoost\n",
    "  - 3.2.4 Stacking\n",
    "- **3.3 k-Nearest-Neighbor**\n",
    "  - 3.3.1 k-Nearest-Neighbor mit euklidischer Metrik\n",
    "  - 3.3.2 k-Nearest-Neighbor mit Manhattan Metrik\n",
    "  - 3.3.3 k-Nearest-Neighbor mit Minkowski Metrik (p = 3)\n",
    "- **3.4 Support Vector Machine**\n",
    "- **3.5 Neuronales Netz**\n",
    "\n",
    "## 4. Bedeutung der einzelnen Features\n",
    "- **4.1 Feature-Bedeutung von PCA**\n",
    "- **4.2 Feature-Bedeutung f√ºr Random Forest**\n",
    "- **4.3 Feature-Bedeutung f√ºr SVM**\n",
    "\n",
    "## 5. Feature-Engineering\n",
    "- **5.1 Generieren der PCA-Hauptkomponenten-Daten**\n",
    "- **5.2 Testen des Feature-Engineering auf k-Nearest-Neighbor mit Manhattan Metrik**\n",
    "- **5.3 Testen des Feature-Engineering auf einem klassischen Entscheidungsbaum**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einbindung der Daten\n",
    "\n",
    "Zu beginn des Projekts werden die Daten zun√§chst geladen um diese im anschluss analysieren und nutzen zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "import plotly.graph_objects as go\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('KHK_Klassifikation.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 explorative Analyse der Daten \n",
    "\n",
    "Die explorative Datenanalyse (EDA) ist ein Ansatz zur Untersuchung von Datens√§tzen, bei dem zun√§chst deren Hauptmerkmale visuell und statistisch beschrieben werden ‚Äì oft noch ohne eine konkrete Hypothese. Ziel ist es, ein erstes Verst√§ndnis f√ºr Struktur, Muster, Ausrei√üer, Verteilungen und potenzielle Zusammenh√§nge in den Daten zu bekommen (vgl. https://www.ibm.com/think/topics/exploratory-data-analysis).\n",
    "\n",
    "### üìÑ Beschreibung der Attribute im Datensatz\n",
    "\n",
    "| Attribut      | Beschreibung |\n",
    "|---------------|-------------|\n",
    "| **Alter** | Alter der Patientin oder des Patienten in Jahren. |\n",
    "| **Geschlecht** | Geschlecht der Person: <br>`M` steht f√ºr m√§nnlich, `F` f√ºr weiblich. |\n",
    "| **Blutdruck** | Systolischer Blutdruck in mmHg (Millimeter Quecksilbers√§ule), gemessen im Ruhezustand. Werte ab 140 gelten in der Regel als erh√∂hter Blutdruck. (vgl. https://www.visomat.de/blutdruck-normalwerte/)|\n",
    "| **Chol** | Gesamtcholesterin im Blut in mg/dL (Milligramm pro Deziliter). Erh√∂hte Werte (>190‚ÄØmg/dL) k√∂nnen ein Risiko f√ºr Herz-Kreislauf-Erkrankungen darstellen. (vgl. https://www.cholesterinspiegel.de/auffaellige-cholesterinwerte/) |\n",
    "| **Blutzucker** | N√ºchtern-Blutzuckerwert: <br>`0` = Normaler Blutzucker <br>`1` = Erh√∂hter Blutzucker (m√∂glicher Hinweis auf Diabetes oder Pr√§diabetes). |\n",
    "| **EKG** | Ergebnis des Ruhe-EKGs. M√∂gliche Kategorien: <br>- `Normal` = unauff√§lliger Befund <br>- `ST` = ST-Streckensenkung (Hinweis auf BelastungsischaÃàmie) <br>- `LVH` = Linksventrikul√§re Hypertrophie (Herzmuskelvergr√∂√üerung). |\n",
    "| **HFmax** | Maximale Herzfrequenz (in Schl√§gen pro Minute), die w√§hrend eines Belastungstests erreicht wurde. Sehr grobe Faustregel: HFmax = 220 - Lebensalter (vgl. https://www.germanjournalsportsmedicine.com/archive/archive-2010/heft-12/die-maximale-herzfrequenz/) |\n",
    "| **AP** | Angina Pectoris bei Belastung: <br>`N` = Keine Symptome <br>`Y` = Auftreten von Angina Pectoris (Brustschmerzen unter Belastung), m√∂glicher Hinweis auf Durchblutungsst√∂rungen des Herzens. |\n",
    "| **RZ** | R√ºckgang (bzw. Ver√§nderung) der ST-Strecke w√§hrend eines Belastungs-EKGs in **mm**. <br> Positive Werte deuten auf eine **ST-Streckensenkung** hin, was auf eine m√∂gliche **Isch√§mie des Herzmuskels** (z.‚ÄØB. bei KHK) hindeuten kann. <br> Negative Werte k√∂nnen als **ST-Streckenhebung** interpretiert werden ‚Äì diese k√∂nnen je nach klinischem Zusammenhang normal, unspezifisch oder auch pathologisch sein (z.‚ÄØB. bei Infarkten oder Perikarditis). <br> In der Regel gilt: Je gr√∂√üer der **absolute Betrag**, desto auff√§lliger der Befund. |\n",
    "| **KHK** | **Zielvariable** ‚Äì Diagnose einer koronaren Herzkrankheit: <br>`0` = Keine KHK <br>`1` = KHK nachgewiesen (positives Ergebnis). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erster Blick auf die Daten:**  \n",
    "Zuerst wird eine Kopie des Datensatzes erstellt und ein erster Blick auf die obersten Zeilen geworfen.\n",
    "Dies dient dazu einen ersten groben √úberblick √ºber die Daten zu bekommen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopie des Original-Datensatzes\n",
    "df = data.copy()\n",
    "\n",
    "# Zeige die ersten paar Zeilen\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Allgemeine Informationen:**  \n",
    "Mit `df.info()` erh√§lt man einen √úberblick √ºber die Spalten, Datentypen und Anzahl fehlender Werte. Das ist wichtig, um zu verstehen, welche Features numerisch oder kategorisch sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allgemeine Infos √ºber den Datensatz\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistische Kennzahlen:**  \n",
    "Diese √úbersicht zeigt zentrale Lage- und Streuungsma√üe (z.‚ÄØB. Mittelwert, Standardabweichung) f√ºr alle numerischen Spalten. Das hilft, Ausrei√üer oder ungew√∂hnliche Verteilungen fr√ºhzeitig zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische √úbersicht √ºber numerische Merkmale\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kategoriale Merkmale analysieren:**  \n",
    "Nun wird die H√§ufigkeit der Werte in allen kategorialen Spalten angesehen. So erkennt man dominante Klassen und m√∂gliche Ungleichgewichte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√§ufigkeit von Werten bei kategorialen Features\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nWertverteilung f√ºr '{col}':\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fehlende Werte pr√ºfen:**  \n",
    "Hier wird analysiert, in welchen Spalten Daten fehlen. Dies ist entscheidend f√ºr die sp√§tere Datenbereinigung oder Modellierung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehlende Werte\n",
    "print(\"\\nFehlende Werte pro Spalte:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doppelte Eintr√§ge identifizieren:**  \n",
    "Es wird gepr√ºft, ob es doppelte Zeilen gibt ‚Äì diese sollten bei Bedarf entfernt werden, um Verzerrungen zu vermeiden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplikate pr√ºfen\n",
    "print(\"\\nAnzahl doppelter Zeilen:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zielvariable analysieren:**  \n",
    "Ein schneller Blick auf die Verteilung der Zielgr√∂√üe (KHK: koronare Herzkrankheit). So sieht man z.‚ÄØB., ob die Klassen unausgeglichen sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Zielvariable (KHK)\n",
    "print(\"\\nVerteilung der Zielvariable 'KHK':\")\n",
    "print(df[\"KHK\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplots zur √úbersicht:**  \n",
    "Boxplots geben einen schnellen √úberblick √ºber die Verteilung numerischer Merkmale inkl. Ausrei√üer. Dies ist besonders hilfreich zum Erkennen von Extremwerten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_cols_filtered = [\n",
    "    col for col in numerical_cols if df[col].nunique() > 2]\n",
    "\n",
    "for col in numerical_cols_filtered:\n",
    "    fig = px.box(df, y=col, points=\"all\",\n",
    "                 title=f\"Boxplot: {col}\", template=\"plotly_white\")\n",
    "    fig.update_layout(yaxis_title=col)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogramme zur Dichteverteilung:**  \n",
    "Diese Plots zeigen, wie sich die Werte in den numerischen Spalten verteilen. Zus√§tzlich ist oben ein Boxplot integriert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols_filtered:\n",
    "    fig = px.histogram(df, x=col, nbins=20, marginal=\"box\",\n",
    "                       title=f\"Histogramm: {col}\", template=\"plotly_white\", color_discrete_sequence=[\"steelblue\"])\n",
    "    fig.update_layout(xaxis_title=col, yaxis_title=\"H√§ufigkeit\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplots nach KHK-Klassen:**  \n",
    "Hier wird analysiert, ob sich bestimmte numerische Merkmale in Abh√§ngigkeit von der Zielvariable signifikant unterscheiden. Das kann Hinweise auf relevante Pr√§diktoren liefern.\n",
    "\n",
    "Wichtig zu erw√§hnen ist, dass viele Chol Werte 0 sind, was auf fehlende Daten hinweist, womit die Chol Statistiken etwas von der realen Lage abweichen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_khk = [\n",
    "    col for col in numerical_cols\n",
    "    if df[col].nunique() > 2 and col != \"KHK\" and col != \"Blutzucker\"\n",
    "]\n",
    "\n",
    "for col in numerical_cols_khk:\n",
    "    fig = px.box(df, x=\"KHK\", y=col, color=\"KHK\",\n",
    "                 title=f\"{col} nach KHK-Klasse\", template=\"plotly_white\", points=\"all\")\n",
    "    fig.update_layout(xaxis_title=\"KHK (0 = Nein, 1 = Ja)\", yaxis_title=col)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise von PCA\n",
    "Die Hauptkomponentenanalyse (PCA) dient der Dimensionsreduktion eines Datensatzes. Dies erm√∂glicht beispielsweise verschiedene Analyse des gesamten Datensatzes (mit mehr als 3 Dimensionen), wobei die Ergebnisse durch die Dimensionsreduktion weiterhin visualisiert werden k√∂nnen.\n",
    "Das Verfahren der PCA l√§uft nach folgendem Schema ab:\n",
    "\n",
    "1. Berechnung des Mittelwerts und Zentrierung der Daten\n",
    "2. Berechnung der Kovarianzmatrix\n",
    "3. Berechnung der Eigenwerte und Eigenvektoren\n",
    "4. Transformation der Daten\n",
    "\n",
    "Damit die PCA korrekt funktioniert, muss zun√§chst von jeder Dimension der Mittelwert subtrahiert werden. Dieser Mittelwert entspricht dem Durchschnittswert jeder Dimension. Beispielsweise wird von allen $x$-Werten der Mittelwert $\\overline{x}$ subtrahiert. Entsprechendes gilt f√ºr die anderen Dimensionen der Daten. Dadurch entsteht ein Datensatz mit einem Mittelwert von null.\n",
    "\n",
    "Im n√§chsten Schritt wird die Kovarianzmatrix berechnet, welche die wechselseitigen Zusammenh√§nge zwischen den Merkmalen quantifiziert. Falls zwei Merkmale stark korrelieren, k√∂nnen diese in einer neuen Achse kombiniert werden.\n",
    "\n",
    "Anschlie√üend werden die Eigenwerte und Eigenvektoren der Kovarianzmatrix bestimmt. Die Eigenvektoren definieren die Richtungen der Hauptkomponenten, w√§hrend die zugeh√∂rigen Eigenwerte die Bedeutung bzw. die Varianz der jeweiligen Eigenvektoren widerspiegeln.\n",
    "\n",
    "Es folgt die eigentliche Dimensionsreduktion, indem nur diejenigen Eigenvektoren mit den gr√∂√üten Eigenwerten ausgew√§hlt werden. Diese Eigenvektoren entsprechen den neuen Hauptachsen des Datensatzes.\n",
    "\n",
    "Schlie√ülich werden die Daten transformiert, indem die urspr√ºngliche Datenmatrix mit der Matrix der Eigenvektoren multipliziert wird. In dieser Matrix repr√§sentiert jede Spalte einen Eigenvektor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['Geschlecht', 'EKG', 'AP']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    # Encode categorical columns\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target variable \"KHK\" before scaling\n",
    "data_without_target = data.drop(columns=[\"KHK\"], errors=\"ignore\")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_without_target)\n",
    "\n",
    "# PCA transformation with two principal components\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Convert the PCA results into a DataFrame\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Interactive visualization\n",
    "fig = px.scatter(df_pca, x='PC1', y='PC2', title='PCA Visualization of the Data', opacity=0.5)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L√§sst sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen?\n",
    "\n",
    "Es sind zwar einzelne \"Flecken\" zu sehen, die etwas konzentriertere Mengen an Punkten darstellen, jedoch ist es schwierig m√∂glich hieraus verschiedene Klassen abzuleiten.\n",
    "Dies liegt vor allem an der Reduktion der Vielzahl an Attributen auf nur 2 Hauptachsen, wobei dann doch zu viele Informationen bei der Darstellung verloren gehen.\n",
    "\n",
    "TODO\n",
    "--> Ich w√ºrde sagen nein, lass aber mal dr√ºber quatschen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anwendung verschiedener vorgestellter Klassifikationsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition und Datenvorbereitung\n",
    "\n",
    "Zun√§chst werden die kategorialen und numerischen Merkmale des Datensatzes definiert. Anschlie√üend erfolgt die Vorbereitung der Daten f√ºr ein Machine-Learning-Modell. Dazu geh√∂ren die Auswahl und Umordnung der Merkmale, die Umwandlung kategorialer Variablen mittels Label-Encoding (vgl. https://kantschants.com/complete-guide-to-encoding-categorical-features), die Standardisierung der numerischen Variablen sowie die Aufteilung in Trainings- und Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical_features = [\"Geschlecht\", \"EKG\", \"AP\"]\n",
    "numerical_features = [\"Alter\", \"Blutdruck\", \"Chol\", \"Blutzucker\", \"HFmax\", \"RZ\"]\n",
    "\n",
    "# Select target variable and features\n",
    "X = data[categorical_features + numerical_features].copy()\n",
    "y = data[\"KHK\"]\n",
    "\n",
    "# Reorder features to match the desired order\n",
    "desired_order = [\"Alter\", \"Geschlecht\", \"Blutdruck\", \"Chol\", \"Blutzucker\", \"EKG\", \"HFmax\", \"AP\", \"RZ\"]\n",
    "X = X[desired_order]\n",
    "\n",
    "# Apply Label Encoding to categorical features\n",
    "label_encoders = {}  # Store LabelEncoder objects\n",
    "for col in categorical_features:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    X[col] = label_encoders[col].fit_transform(X[col])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistische Regression\n",
    "Logistische Regression ist ein statistisches Modell, das den nat√ºrlichen Logarithmus der Chancen eines Ereignisses als lineare Kombination einer oder mehrerer unabh√§ngiger Variablen modelliert.\n",
    "In der Regressionsanalyse sch√§tzt die logistische Regression die Parameter dieses Modells, typischerweise in Szenarien mit einer bin√§ren Zielvariablen (z.‚ÄØB. 0 oder 1) (vgl. https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "Logistische Regression passt gut zu dem Datensatz, da KHK bin√§r ist, der Datensatz gemischte Merkmale enth√§lt und das Modell wahrscheinlichkeiten f√ºr KHK einfach und interpretierbar berechnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Modell definieren und trainieren\n",
    "\n",
    "Das beschriebene logistische Regressionsmodell wird erstellt und mit den Trainingsdaten trainiert, um KHK als bin√§re Zielvariable vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression for binary classification\n",
    "\n",
    "# Create pipeline with preprocessing and logistic regression\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Modell testen\n",
    "\n",
    "Das trainierte Modell wird auf den Testdaten angewendet. Die Auswertung erfolgt anhand der **Genauigkeit (Accuracy)** und eines **Classification Reports**, der die wichtigsten Metriken zur Bewertung der Vorhersagequalit√§t enth√§lt:\n",
    "\n",
    "| **Metrik**     | **Beschreibung**                                                                 |\n",
    "|----------------|-----------------------------------------------------------------------------------|\n",
    "| **Accuracy**   | Anteil korrekt klassifizierter Beispiele an allen Beispielen                     |\n",
    "| **Precision**  | Anteil korrekt positiver Vorhersagen an allen als positiv vorhergesagten F√§llen (vgl. https://www.v7labs.com/blog/precision-vs-recall-guide)  |\n",
    "| **Recall**     | Anteil korrekt erkannter positiver F√§lle an allen tats√§chlichen positiven F√§llen (vgl. https://www.v7labs.com/blog/precision-vs-recall-guide) |\n",
    "| **F1-Score**   | Harmonisches Mittel aus Precision und Recall (balanciert beide Metriken) (vgl. https://www.v7labs.com/blog/f1-score-guide)       |\n",
    "\n",
    "Diese Metriken geben gemeinsam ein gutes Bild dar√ºber, wie zuverl√§ssig das Modell bei der KHK-Klassifikation arbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_log_reg = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "classification_rep = classification_report(y_test, y_pred_log_reg)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entscheidungsb√§ume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 klassische Entscheidungsb√§ume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier\n",
    "clf_tree = DecisionTreeClassifier(random_state=42)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "classification_rep_tree = classification_report(y_test, y_pred_tree)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_tree:.2f}\")\n",
    "print(classification_rep_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Bagging in Form von Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_random_forest = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "classification_rep = classification_report(y_test, y_pred_random_forest)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Boosting in Form von AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Train AdaBoost model with specified parameters\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_ada = adaboost_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_ada)  # Note: variable name might be misleading\n",
    "classification_rep = classification_report(y_test, y_pred_ada)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models: KNN, SVM, and Logistic Regression\n",
    "base_estimators = [\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),  # KNN with 5 neighbors\n",
    "    ('svc', SVC(kernel='linear', random_state=42)),  # SVM with a linear kernel\n",
    "    ('logreg', LogisticRegression(random_state=42))  # Logistic Regression\n",
    "]\n",
    "\n",
    "# Final model (meta-model)\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# Create StackingClassifier with base models and final estimator\n",
    "stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=final_estimator)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "classification_rep = classification_report(y_test, y_pred_stack)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_stack:.2f}\")\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 k-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 k-Nearest-Neighbor mit euklidischer Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN model with k=10 and Euclidean distance metric\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 k-Nearest-Neighbor mit manhattan Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN model with k=10 and Manhattan distance metric\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='manhattan')\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 k-Nearest-Neighbor mit Minkowski Metrik und p = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN model with k=10 and Minkowski distance metric (p=3)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p=3)\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model accuracy: {accuracy_svm:.2f}\")\n",
    "print(classification_rep_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # First hidden layer\n",
    "        Dense(32, activation='relu'),  # Second hidden layer\n",
    "        Dense(16, activation='relu'),  # Third hidden layer\n",
    "        Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer,  # Set optimizer\n",
    "                  loss='binary_crossentropy',  # Loss function for binary classification\n",
    "                  metrics=['accuracy'])  # Metrics to track during training\n",
    "\n",
    "    # Display model summary\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the SGD optimizer\n",
    "sgd_model = create_model(optimizer='sgd')\n",
    "\n",
    "# Train the model\n",
    "history_sgd = sgd_model.fit(X_train, y_train,\n",
    "                            epochs=50,  # Number of epochs for training\n",
    "                            batch_size=32,  # Batch size for training\n",
    "                            validation_split=0.2,  # Split of training data for validation\n",
    "                            verbose=1)  # Display progress during training\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss_sgd, test_accuracy_sgd = sgd_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Visualize training history with Plotly\n",
    "\n",
    "# Plot Accuracy\n",
    "fig_accuracy = go.Figure()\n",
    "fig_accuracy.add_trace(go.Scatter(x=list(range(1, 51)), y=history_sgd.history['accuracy'],\n",
    "                                 mode='lines', name='Training Accuracy'))\n",
    "fig_accuracy.add_trace(go.Scatter(x=list(range(1, 51)), y=history_sgd.history['val_accuracy'],\n",
    "                                 mode='lines', name='Validation Accuracy'))\n",
    "\n",
    "fig_accuracy.update_layout(\n",
    "    title='Model Accuracy',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Accuracy'\n",
    ")\n",
    "\n",
    "# Plot Loss\n",
    "fig_loss = go.Figure()\n",
    "fig_loss.add_trace(go.Scatter(x=list(range(1, 51)), y=history_sgd.history['loss'],\n",
    "                             mode='lines', name='Training Loss'))\n",
    "fig_loss.add_trace(go.Scatter(x=list(range(1, 51)), y=history_sgd.history['val_loss'],\n",
    "                             mode='lines', name='Validation Loss'))\n",
    "\n",
    "fig_loss.update_layout(\n",
    "    title='Model Loss',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss'\n",
    ")\n",
    "\n",
    "# Show the figures\n",
    "fig_accuracy.show()\n",
    "fig_loss.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "y_pred_classes_sgd = (y_pred_sgd > 0.5).astype(int)  # Convert probabilities to binary classes\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the model using the Adam optimizer\n",
    "adam_model = create_model(optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history_adam = adam_model.fit(X_train, y_train,\n",
    "                              epochs=50,  # Number of epochs for training\n",
    "                              batch_size=32,  # Batch size for training\n",
    "                              validation_split=0.2,  # Split of training data for validation\n",
    "                              verbose=1)  # Display progress during training\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy_adam = adam_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Visualize training history with Plotly\n",
    "\n",
    "# Plot Accuracy\n",
    "fig_accuracy_adam = go.Figure()\n",
    "fig_accuracy_adam.add_trace(go.Scatter(x=list(range(1, 51)), y=history_adam.history['accuracy'],\n",
    "                                      mode='lines', name='Training Accuracy'))\n",
    "fig_accuracy_adam.add_trace(go.Scatter(x=list(range(1, 51)), y=history_adam.history['val_accuracy'],\n",
    "                                      mode='lines', name='Validation Accuracy'))\n",
    "\n",
    "fig_accuracy_adam.update_layout(\n",
    "    title='Model Accuracy',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Accuracy'\n",
    ")\n",
    "\n",
    "# Plot Loss\n",
    "fig_loss_adam = go.Figure()\n",
    "fig_loss_adam.add_trace(go.Scatter(x=list(range(1, 51)), y=history_adam.history['loss'],\n",
    "                                  mode='lines', name='Training Loss'))\n",
    "fig_loss_adam.add_trace(go.Scatter(x=list(range(1, 51)), y=history_adam.history['val_loss'],\n",
    "                                  mode='lines', name='Validation Loss'))\n",
    "\n",
    "fig_loss_adam.update_layout(\n",
    "    title='Model Loss',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss'\n",
    ")\n",
    "\n",
    "# Show the figures\n",
    "fig_accuracy_adam.show()\n",
    "fig_loss_adam.show()\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adam = adam_model.predict(X_test)\n",
    "y_pred_classes_adam = (y_pred_adam > 0.5).astype(int)  # Convert probabilities to binary classes\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_adam))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bedeutung der einzelnen Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature-Bedeutung von PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (excluding the target variable \"KHK\")\n",
    "feature_names = data.columns.tolist()\n",
    "feature_names.remove(\"KHK\")\n",
    "\n",
    "# Compute the importance of features from PCA components\n",
    "feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "\n",
    "# Create DataFrame for Plotly visualization\n",
    "df_plot = pd.DataFrame({\"Feature\": feature_names, \"Wichtigkeit\": feature_importance})\n",
    "\n",
    "# Create an interactive bar plot with Plotly\n",
    "fig = px.bar(df_plot, x=\"Feature\", y=\"Wichtigkeit\", title=\"Feature Importance from PCA\", labels={\"Feature\": \"Feature\", \"Wichtigkeit\": \"Feature Importance\"})\n",
    "fig.update_xaxes()  # Update x-axis for better readability\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature-Bedeutung f√ºr Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance from the Random Forest model\n",
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Create DataFrame for Plotly visualization\n",
    "df_plot = pd.DataFrame({\"Feature\": X.columns.tolist(), \"Wichtigkeit\": feature_importance})\n",
    "\n",
    "# Create an interactive bar plot with Plotly\n",
    "fig = px.bar(df_plot, x=\"Feature\", y=\"Wichtigkeit\", title=\"Feature Importance from Random Forest\", labels={\"Feature\": \"Feature\", \"Wichtigkeit\": \"Feature Importance\"})\n",
    "fig.update_xaxes()  # Update x-axis for better readability\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Feature Bedeutung SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute values of the coefficients as feature importance\n",
    "feature_importance = abs(svm_model.coef_).flatten()\n",
    "\n",
    "# Create DataFrame for Plotly visualization\n",
    "df_plot = pd.DataFrame({\"Feature\": X.columns.tolist(), \"Wichtigkeit\": feature_importance})\n",
    "\n",
    "# Create an interactive bar plot with Plotly\n",
    "fig = px.bar(df_plot, x=\"Feature\", y=\"Wichtigkeit\", title=\"Feature Importance from SVM\", labels={\"Feature\": \"Feature\", \"Wichtigkeit\": \"Feature Importance\"})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature-Engineering\n",
    "\n",
    "F√ºr das Feature Engineering wurden zwei Klassifikationsverfahren ausgew√§hlt. Einmal wurde k-Nearest-Neighbor mit Manhattan Metrik genutzt und zus√§tzlich klassische Entscheidungsb√§ume. Diese beiden Klassifikationsverfahren wurden ausgew√§hlt, das k-Nearest-Neighbor mit Manhattan Metrik beim testen die h√∂chste und klassische Entscheidungsb√§ume die schlechteste Genauigkeit hatten. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Generieren der PCA-Hauptkomponenten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of principal components to keep (can be adjusted)\n",
    "pca_components = 2\n",
    "\n",
    "# Perform PCA transformation with the specified number of components\n",
    "pca = PCA(n_components=pca_components)\n",
    "X_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Convert the PCA results into a DataFrame\n",
    "df_pca = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(pca_components)])\n",
    "\n",
    "# Add the target variable \"KHK\" to the PCA DataFrame\n",
    "df_pca['KHK'] = data['KHK'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(df_pca.drop(columns=[\"KHK\"]), df_pca[\"KHK\"], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Testen des Feature-Engineering auf k-Nearest-Neighbor mit Manhattan Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-NN model with k=10 for PCA features using Manhattan distance\n",
    "knn_model_pca = KNeighborsClassifier(n_neighbors=10, metric='manhattan')\n",
    "\n",
    "# Train the model on PCA-transformed features\n",
    "knn_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_knn_pca = knn_model_pca.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_knn_pca = accuracy_score(y_test, y_pred_knn_pca)\n",
    "classification_rep_knn_pca = classification_report(y_test, y_pred_knn_pca)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Model accuracy: {accuracy_knn_pca:.2f}\")\n",
    "print(classification_rep_knn_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Testen des Feature-Engineering auf einem klassischen Entscheidungsbaum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree model for PCA features\n",
    "clf_tree_pca = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on PCA-transformed features\n",
    "clf_tree_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_tree_pca = clf_tree_pca.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tree_pca = accuracy_score(y_test, y_pred_tree_pca)\n",
    "classification_rep_tree_pca = classification_report(y_test, y_pred_tree_pca)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f\"Model accuracy: {accuracy_tree_pca:.2f}\")\n",
    "print(classification_rep_tree_pca)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
