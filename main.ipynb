{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendung von maschinellem Lernen auf den KHK_Klassifikation.csv Datensatz\n",
    "\n",
    "## Praktische Demonstration f√ºr verschiedene machine Learning Modelle\n",
    "\n",
    "### Tim Bleicher, Linus Pfeifer\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert die Anwendung von verschiedenen Machine Learning Modellen auf den KHK_Klassifikation.csv Datensatz. \n",
    "\n",
    "**Inhaltsverzeichnis:**\n",
    "- [1. Einbindung der Daten](#1-einbindung-der-daten)\n",
    "  - [1.1 explorative Analyse der Daten](#11-explorative-analyse-der-daten)\n",
    "- [2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten](#2-pca-dimensionsreduzierung-zur-visualisierung-und-analyse-der-daten)\n",
    "  - [Funktionsweise von PCA](#funktionsweise-von-pca)\n",
    "  - [L√§sst sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen?](#l√§sst-sich-aus-den-pca-daten-eine-potentielle-gute-separierbarkeit-der-klassen-ablesen)\n",
    "- [3. Anwendung verschiedener vorgestellter Klassifikationsverfahren](#3-anwendung-verschiedener-vorgestellter-klassifikationsverfahren)\n",
    "  - [Definition und Datenvorbereitung](#definition-und-datenvorbereitung)\n",
    "  - [3.1 logistische Regression](#31-logistische-regression)\n",
    "    - [Modell definieren und trainieren](#modell-definieren-und-trainieren)\n",
    "    - [Modell testen](#modell-testen)\n",
    "  - [3.2 Entscheidungsb√§ume](#32-entscheidungsb√§ume)\n",
    "    - [3.2.1 klassische Entscheidungsb√§ume](#321-klassische-entscheidungsb√§ume)\n",
    "    - [3.2.2 Bagging in Form von Random Forest](#322-bagging-in-form-von-random-forest)\n",
    "    - [3.2.3 Boosting in Form von AdaBoost](#323-boosting-in-form-von-adaboost)\n",
    "    - [3.2.4 Stacking](#324-stacking)\n",
    "  - [3.3 k-Nearest-Neighbor](#33-k-nearest-neighbor)\n",
    "    - [3.3.1 k-Nearest-Neighbor mit euklidischer Metrik](#331-k-nearest-neighbor-mit-euklidischer-metrik)\n",
    "    - [3.3.2 k-Nearest-Neighbor mit manhattan Metrik](#332-k-nearest-neighbor-mit-manhattan-metrik)\n",
    "    - [3.3.4 k-Nearest-Neighbor mit Minkowski Metrik und p = 3](#334-k-nearest-neighbor-mit-minkowski-metrik-und-p--3)\n",
    "  - [3.4 Support Vector Machine](#34-support-vector-machine)\n",
    "  - [3.5 Neuronales Netz](#35-neuronales-netz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einbindung der Daten\n",
    "\n",
    "Zu beginn des Projekts werden die Daten zun√§chst geladen um diese im anschluss analysieren und nutzen zu k√∂nnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('KHK_Klassifikation.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 explorative Analyse der Daten \n",
    "\n",
    "Die explorative Datenanalyse (EDA) ist ein Ansatz zur Untersuchung von Datens√§tzen, bei dem zun√§chst deren Hauptmerkmale visuell und statistisch beschrieben werden ‚Äì oft noch ohne eine konkrete Hypothese. Ziel ist es, ein erstes Verst√§ndnis f√ºr Struktur, Muster, Ausrei√üer, Verteilungen und potenzielle Zusammenh√§nge in den Daten zu bekommen (vgl. https://www.ibm.com/think/topics/exploratory-data-analysis).\n",
    "\n",
    "### üìÑ Beschreibung der Attribute im Datensatz\n",
    "\n",
    "| Attribut      | Beschreibung |\n",
    "|---------------|-------------|\n",
    "| **Alter** | Alter der Patientin oder des Patienten in Jahren. |\n",
    "| **Geschlecht** | Geschlecht der Person: <br>`M` steht f√ºr m√§nnlich, `F` f√ºr weiblich. |\n",
    "| **Blutdruck** | Systolischer Blutdruck in mmHg (Millimeter Quecksilbers√§ule), gemessen im Ruhezustand. Werte ab 140 gelten in der Regel als erh√∂hter Blutdruck. (vgl. https://www.visomat.de/blutdruck-normalwerte/)|\n",
    "| **Chol** | Gesamtcholesterin im Blut in mg/dL (Milligramm pro Deziliter). Erh√∂hte Werte (>190‚ÄØmg/dL) k√∂nnen ein Risiko f√ºr Herz-Kreislauf-Erkrankungen darstellen. (vgl. https://www.cholesterinspiegel.de/auffaellige-cholesterinwerte/) |\n",
    "| **Blutzucker** | N√ºchtern-Blutzuckerwert: <br>`0` = Normaler Blutzucker <br>`1` = Erh√∂hter Blutzucker (m√∂glicher Hinweis auf Diabetes oder Pr√§diabetes). |\n",
    "| **EKG** | Ergebnis des Ruhe-EKGs. M√∂gliche Kategorien: <br>- `Normal` = unauff√§lliger Befund <br>- `ST` = ST-Streckensenkung (Hinweis auf BelastungsischaÃàmie) <br>- `LVH` = Linksventrikul√§re Hypertrophie (Herzmuskelvergr√∂√üerung). |\n",
    "| **HFmax** | Maximale Herzfrequenz (in Schl√§gen pro Minute), die w√§hrend eines Belastungstests erreicht wurde. Sehr grobe Faustregel: HFmax = 220 - Lebensalter (vgl. https://www.germanjournalsportsmedicine.com/archive/archive-2010/heft-12/die-maximale-herzfrequenz/) |\n",
    "| **AP** | Angina Pectoris bei Belastung: <br>`N` = Keine Symptome <br>`Y` = Auftreten von Angina Pectoris (Brustschmerzen unter Belastung), m√∂glicher Hinweis auf Durchblutungsst√∂rungen des Herzens. |\n",
    "| **RZ** | R√ºckgang (bzw. Ver√§nderung) der ST-Strecke w√§hrend eines Belastungs-EKGs in **mm**. <br> Positive Werte deuten auf eine **ST-Streckensenkung** hin, was auf eine m√∂gliche **Isch√§mie des Herzmuskels** (z.‚ÄØB. bei KHK) hindeuten kann. <br> Negative Werte k√∂nnen als **ST-Streckenhebung** interpretiert werden ‚Äì diese k√∂nnen je nach klinischem Zusammenhang normal, unspezifisch oder auch pathologisch sein (z.‚ÄØB. bei Infarkten oder Perikarditis). <br> In der Regel gilt: Je gr√∂√üer der **absolute Betrag**, desto auff√§lliger der Befund. |\n",
    "| **KHK** | **Zielvariable** ‚Äì Diagnose einer koronaren Herzkrankheit: <br>`0` = Keine KHK <br>`1` = KHK nachgewiesen (positives Ergebnis). |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1. Daten laden und √úberblick gewinnen\n",
    "# ========================================\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# Zeige die ersten paar Zeilen\n",
    "display(df.head())\n",
    "\n",
    "# Allgemeine Infos √ºber den Datensatz\n",
    "display(df.info())\n",
    "\n",
    "# Statistische √úbersicht √ºber numerische Merkmale\n",
    "display(df.describe())\n",
    "\n",
    "# H√§ufigkeit von Werten bei kategorialen Features\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\nWertverteilung f√ºr '{col}':\")\n",
    "    print(df[col].value_counts())\n",
    "\n",
    "# Fehlende Werte\n",
    "print(\"\\nFehlende Werte pro Spalte:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Duplikate pr√ºfen\n",
    "print(\"\\nAnzahl doppelter Zeilen:\", df.duplicated().sum())\n",
    "\n",
    "# Verteilung der Zielvariable (KHK)\n",
    "print(\"\\nVerteilung der Zielvariable 'KHK':\")\n",
    "print(df[\"KHK\"].value_counts())\n",
    "\n",
    "# ========================================\n",
    "# 2. Visualisierung ‚Äì Boxplots (Plotly)\n",
    "# ========================================\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_cols_filtered = [col for col in numerical_cols if df[col].nunique() > 2]\n",
    "\n",
    "for col in numerical_cols_filtered:\n",
    "    fig = px.box(df, y=col, points=\"all\", title=f\"Boxplot: {col}\", template=\"plotly_white\")\n",
    "    fig.update_layout(yaxis_title=col)\n",
    "    fig.show()\n",
    "\n",
    "# ========================================\n",
    "# 3. Visualisierung ‚Äì Histogramme (Plotly)\n",
    "# ========================================\n",
    "\n",
    "for col in numerical_cols_filtered:\n",
    "    fig = px.histogram(df, x=col, nbins=20, marginal=\"box\", title=f\"Histogramm: {col}\", template=\"plotly_white\", color_discrete_sequence=[\"steelblue\"])\n",
    "    fig.update_layout(xaxis_title=col, yaxis_title=\"H√§ufigkeit\")\n",
    "    fig.show()\n",
    "\n",
    "# ========================================\n",
    "# 4. Vergleich nach KHK ‚Äì Boxplots (Plotly)\n",
    "# ========================================\n",
    "\n",
    "numerical_cols_khk = [\n",
    "    col for col in numerical_cols\n",
    "    if df[col].nunique() > 2 and col != \"KHK\" and col != \"Blutzucker\"\n",
    "]\n",
    "\n",
    "for col in numerical_cols_khk:\n",
    "    fig = px.box(df, x=\"KHK\", y=col, color=\"KHK\", title=f\"{col} nach KHK-Klasse\", template=\"plotly_white\", points=\"all\")\n",
    "    fig.update_layout(xaxis_title=\"KHK (0 = Nein, 1 = Ja)\", yaxis_title=col)\n",
    "    fig.show()\n",
    "\n",
    "# ========================================\n",
    "# 5. Umwandlung der nicht-numerischen Werte\n",
    "# ========================================\n",
    "\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Bin√§re Umwandlung\n",
    "df_encoded[\"Geschlecht\"] = df_encoded[\"Geschlecht\"].map({\"M\": 0, \"F\": 1})\n",
    "df_encoded[\"AP\"] = df_encoded[\"AP\"].map({\"N\": 0, \"Y\": 1})\n",
    "\n",
    "# One-Hot-Encoding f√ºr EKG\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=[\"EKG\"], drop_first=True)\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(\"\\nDaten nach Umkodierung:\")\n",
    "display(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise von PCA\n",
    "Die Hauptkomponentenanalyse (PCA) dient der Dimensionsreduktion eines Datensatzes. Dies erm√∂glicht beispielsweise verschiedene Analyse des gesamten Datensatzes (mit mehr als 3 Dimensionen), wobei die Ergebnisse durch die Dimensionsreduktion weiterhin visualisiert werden k√∂nnen.\n",
    "Das Verfahren der PCA l√§uft nach folgendem Schema ab:\n",
    "\n",
    "1. Berechnung des Mittelwerts und Zentrierung der Daten\n",
    "2. Berechnung der Kovarianzmatrix\n",
    "3. Berechnung der Eigenwerte und Eigenvektoren\n",
    "4. Transformation der Daten\n",
    "\n",
    "Damit die PCA korrekt funktioniert, muss zun√§chst von jeder Dimension der Mittelwert subtrahiert werden. Dieser Mittelwert entspricht dem Durchschnittswert jeder Dimension. Beispielsweise wird von allen $x$-Werten der Mittelwert $\\overline{x}$ subtrahiert. Entsprechendes gilt f√ºr die anderen Dimensionen der Daten. Dadurch entsteht ein Datensatz mit einem Mittelwert von null.\n",
    "\n",
    "Im n√§chsten Schritt wird die Kovarianzmatrix berechnet, welche die wechselseitigen Zusammenh√§nge zwischen den Merkmalen quantifiziert. Falls zwei Merkmale stark korrelieren, k√∂nnen diese in einer neuen Achse kombiniert werden.\n",
    "\n",
    "Anschlie√üend werden die Eigenwerte und Eigenvektoren der Kovarianzmatrix bestimmt. Die Eigenvektoren definieren die Richtungen der Hauptkomponenten, w√§hrend die zugeh√∂rigen Eigenwerte die Bedeutung bzw. die Varianz der jeweiligen Eigenvektoren widerspiegeln.\n",
    "\n",
    "Es folgt die eigentliche Dimensionsreduktion, indem nur diejenigen Eigenvektoren mit den gr√∂√üten Eigenwerten ausgew√§hlt werden. Diese Eigenvektoren entsprechen den neuen Hauptachsen des Datensatzes.\n",
    "\n",
    "Schlie√ülich werden die Daten transformiert, indem die urspr√ºngliche Datenmatrix mit der Matrix der Eigenvektoren multipliziert wird. In dieser Matrix repr√§sentiert jede Spalte einen Eigenvektor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['Geschlecht', 'EKG', 'AP']\n",
    "for col in categorical_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entferne die Zielvariable \"KHK\" vor der Skalierung\n",
    "data_without_target = data.drop(columns=[\"KHK\"], errors=\"ignore\")\n",
    "\n",
    "# Skalierung der Daten\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_without_target)\n",
    "\n",
    "# PCA-Transformation mit zwei Hauptkomponenten\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Umwandlung der PCA-Ergebnisse in einen DataFrame\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Interaktive Visualisierung\n",
    "fig = px.scatter(df_pca, x='PC1', y='PC2', title='PCA-Visualisierung der Daten', opacity=0.5)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L√§sst sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen? \n",
    "TODO\n",
    "--> Ich w√ºrde sagen nein, lass aber mal dr√ºber quatschen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anwendung verschiedener vorgestellter Klassifikationsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition und Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorische und numerische Spalten definieren\n",
    "categorical_features = [\"Geschlecht\", \"EKG\", \"AP\"]\n",
    "numerical_features = [\"Alter\", \"Blutdruck\", \"Chol\", \"Blutzucker\", \"HFmax\", \"RZ\"]\n",
    "\n",
    "# Zielvariable und Features ausw√§hlen\n",
    "X = data[categorical_features + numerical_features]\n",
    "y = data[\"KHK\"]\n",
    "\n",
    "# One-Hot-Encoding f√ºr kategorische Variablen\n",
    "X = pd.get_dummies(X, columns=categorical_features)\n",
    "\n",
    "# Standardisierung f√ºr numerische Variablen\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 logistische Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell definieren und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression f√ºr bin√§re Klassifikation\n",
    "\n",
    "# Pipeline mit Vorverarbeitung und logistische Regression\n",
    "model = LogisticRegression()\n",
    "# Modell trainieren\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen treffen\n",
    "y_pred_log_reg = model.predict(X_test)\n",
    "\n",
    "# Evaluierung\n",
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "classification_rep = classification_report(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entscheidungsb√§ume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 klassische Entscheidungsb√§ume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = DecisionTreeClassifier(random_state=42)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "classification_rep_tree = classification_report(y_test, y_pred_tree)\n",
    "print(f\"Modellgenauigkeit: {accuracy_tree:.2f}\")\n",
    "print(classification_rep_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Bagging in Form von Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Modell trainieren\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_random_forest = clf.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "classification_rep = classification_report(y_test, y_pred_random_forest)\n",
    "print(f\"Modellgenauigkeit: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Boosting in Form von AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_ada = adaboost_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_ada)\n",
    "classification_rep = classification_report(y_test, y_pred_ada)\n",
    "print(f\"Modellgenauigkeit: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basismodelle: KNN, SVM und Logistische Regression\n",
    "base_estimators = [\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),  # KNN mit 5 Nachbarn\n",
    "    ('svc', SVC(kernel='linear', random_state=42)),  # SVM mit linearem Kernel\n",
    "    ('logreg', LogisticRegression(random_state=42))  # Logistische Regression\n",
    "]\n",
    "\n",
    "# Finales Modell (Meta-Modell)\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# StackingClassifier erstellen\n",
    "stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=final_estimator)\n",
    "\n",
    "# Modell trainieren\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "classification_rep = classification_report(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_stack:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 k-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 k-Nearest-Neighbor mit euklidischer Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 k-Nearest-Neighbor mit manhattan Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='manhattan')\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 k-Nearest-Neighbor mit Minkowski Metrik und p = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p=3)\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Modell trainieren\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_svm:.2f}\")\n",
    "print(classification_rep_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Modell kompilieren\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Modell√ºbersicht anzeigen\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model = create_model(optimizer='sgd')\n",
    "# Modell trainieren\n",
    "history_sgd = sgd_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss_sgd, test_accuracy_sgd = sgd_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Trainingsverlauf visualisieren\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_sgd.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_sgd.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_sgd.history['loss'], label='Training Loss')\n",
    "plt.plot(history_sgd.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "y_pred_classes_sgd = (y_pred_sgd > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix und Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = create_model(optimizer='adam')\n",
    "# Modell trainieren\n",
    "history_adam = adam_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy_adam = adam_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Trainingsverlauf visualisieren\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_adam.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_adam.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_adam.history['loss'], label='Training Loss')\n",
    "plt.plot(history_adam.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred_adam = adam_model.predict(X_test)\n",
    "y_pred_classes_adam = (y_pred_adam > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix und Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_adam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bedeutung der einzelnen Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature-Bedeutung von PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = data.columns.tolist()\n",
    "feature_names.remove(\"KHK\") \n",
    "# Bedeutung der urspr√ºnglichen Features\n",
    "feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "# DataFrame f√ºr Plotly erstellen\n",
    "df_plot = pd.DataFrame({\"Feature\": feature_names, \"Wichtigkeit\": feature_importance})\n",
    "\n",
    "# Interaktive Visualisierung mit Plotly\n",
    "fig = px.bar(df_plot, x=\"Feature\", y=\"Wichtigkeit\", title=\"Feature-Bedeutung aus PCA\", labels={\"Feature\": \"Feature\", \"Wichtigkeit\": \"Feature-Wichtigkeit\"})\n",
    "fig.update_xaxes() \n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
