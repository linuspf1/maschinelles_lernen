{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anwendung von maschinellem Lernen auf den KHK_Klassifikation.csv Datensatz\n",
    "\n",
    "## Praktische Demonstration für verschiedene machine Learning Modelle\n",
    "\n",
    "### Tim Bleicher, Linus Pfeifer\n",
    "\n",
    "Dieses Jupyter Notebook demonstriert die Anwendung von verschiedenen Machine Learning Modellen auf den KHK_Klassifikation.csv Datensatz. \n",
    "\n",
    "**Inhaltsverzeichnis:**\n",
    "- [1. Einbindung der Daten](#1-einbindung-der-daten)\n",
    "  - [1.1 explorative Analyse der Daten](#11-explorative-analyse-der-daten)\n",
    "- [2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten](#2-pca-dimensionsreduzierung-zur-visualisierung-und-analyse-der-daten)\n",
    "  - [Funktionsweise von PCA](#funktionsweise-von-pca)\n",
    "  - [Lässt sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen?](#lässt-sich-aus-den-pca-daten-eine-potentielle-gute-separierbarkeit-der-klassen-ablesen)\n",
    "- [3. Anwendung verschiedener vorgestellter Klassifikationsverfahren](#3-anwendung-verschiedener-vorgestellter-klassifikationsverfahren)\n",
    "  - [Definition und Datenvorbereitung](#definition-und-datenvorbereitung)\n",
    "  - [3.1 logistische Regression](#31-logistische-regression)\n",
    "    - [Modell definieren und trainieren](#modell-definieren-und-trainieren)\n",
    "    - [Modell testen](#modell-testen)\n",
    "  - [3.2 Entscheidungsbäume](#32-entscheidungsbäume)\n",
    "    - [3.2.1 klassische Entscheidungsbäume](#321-klassische-entscheidungsbäume)\n",
    "    - [3.2.2 Bagging in Form von Random Forest](#322-bagging-in-form-von-random-forest)\n",
    "    - [3.2.3 Boosting in Form von AdaBoost](#323-boosting-in-form-von-adaboost)\n",
    "    - [3.2.4 Stacking](#324-stacking)\n",
    "  - [3.3 k-Nearest-Neighbor](#33-k-nearest-neighbor)\n",
    "    - [3.3.1 k-Nearest-Neighbor mit euklidischer Metrik](#331-k-nearest-neighbor-mit-euklidischer-metrik)\n",
    "    - [3.3.2 k-Nearest-Neighbor mit manhattan Metrik](#332-k-nearest-neighbor-mit-manhattan-metrik)\n",
    "    - [3.3.4 k-Nearest-Neighbor mit Minkowski Metrik und p = 3](#334-k-nearest-neighbor-mit-minkowski-metrik-und-p--3)\n",
    "  - [3.4 Support Vector Machine](#34-support-vector-machine)\n",
    "  - [3.5 Neuronales Netz](#35-neuronales-netz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einbindung der Daten\n",
    "\n",
    "Zu beginn des Projekts werden die Daten zunächst geladen um diese im anschluss analysieren und nutzen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('KHK_Klassifikation.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 explorative Analyse der Daten \n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA-Dimensionsreduzierung zur Visualisierung und Analyse der Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktionsweise von PCA\n",
    "Die Hauptkomponentenanalyse (PCA) dient der Dimensionsreduktion eines Datensatzes. Dies ermöglicht beispielsweise verschiedene Analyse des gesamten Datensatzes (mit mehr als 3 Dimensionen), wobei die Ergebnisse durch die Dimensionsreduktion weiterhin visualisiert werden können.\n",
    "Das Verfahren der PCA läuft nach folgendem Schema ab:\n",
    "\n",
    "1. Berechnung des Mittelwerts und Zentrierung der Daten\n",
    "2. Berechnung der Kovarianzmatrix\n",
    "3. Berechnung der Eigenwerte und Eigenvektoren\n",
    "4. Transformation der Daten\n",
    "\n",
    "Damit die PCA korrekt funktioniert, muss zunächst von jeder Dimension der Mittelwert subtrahiert werden. Dieser Mittelwert entspricht dem Durchschnittswert jeder Dimension. Beispielsweise wird von allen $x$-Werten der Mittelwert $\\overline{x}$ subtrahiert. Entsprechendes gilt für die anderen Dimensionen der Daten. Dadurch entsteht ein Datensatz mit einem Mittelwert von null.\n",
    "\n",
    "Im nächsten Schritt wird die Kovarianzmatrix berechnet, welche die wechselseitigen Zusammenhänge zwischen den Merkmalen quantifiziert. Falls zwei Merkmale stark korrelieren, können diese in einer neuen Achse kombiniert werden.\n",
    "\n",
    "Anschließend werden die Eigenwerte und Eigenvektoren der Kovarianzmatrix bestimmt. Die Eigenvektoren definieren die Richtungen der Hauptkomponenten, während die zugehörigen Eigenwerte die Bedeutung bzw. die Varianz der jeweiligen Eigenvektoren widerspiegeln.\n",
    "\n",
    "Es folgt die eigentliche Dimensionsreduktion, indem nur diejenigen Eigenvektoren mit den größten Eigenwerten ausgewählt werden. Diese Eigenvektoren entsprechen den neuen Hauptachsen des Datensatzes.\n",
    "\n",
    "Schließlich werden die Daten transformiert, indem die ursprüngliche Datenmatrix mit der Matrix der Eigenvektoren multipliziert wird. In dieser Matrix repräsentiert jede Spalte einen Eigenvektor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['Geschlecht', 'EKG', 'AP']\n",
    "for col in categorical_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalierung der Daten\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# PCA-Transformation mit zwei Hauptkomponenten\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(data_scaled)\n",
    "df_pca = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Interaktive Visualisierung\n",
    "fig = px.scatter(df_pca, x='PC1', y='PC2', title='PCA-Visualisierung der Daten', opacity=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lässt sich aus den PCA-Daten eine potentielle gute Separierbarkeit der Klassen ablesen? \n",
    "TODO\n",
    "--> Ich würde sagen nein, lass aber mal drüber quatschen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anwendung verschiedener vorgestellter Klassifikationsverfahren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition und Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kategorische und numerische Spalten definieren\n",
    "categorical_features = [\"Geschlecht\", \"EKG\", \"AP\"]\n",
    "numerical_features = [\"Alter\", \"Blutdruck\", \"Chol\", \"Blutzucker\", \"HFmax\", \"RZ\"]\n",
    "\n",
    "# Zielvariable und Features auswählen\n",
    "X = data[categorical_features + numerical_features]\n",
    "y = data[\"KHK\"]\n",
    "\n",
    "# One-Hot-Encoding für kategorische Variablen\n",
    "X = pd.get_dummies(X, columns=categorical_features)\n",
    "\n",
    "# Standardisierung für numerische Variablen\n",
    "scaler = StandardScaler()\n",
    "X[numerical_features] = scaler.fit_transform(X[numerical_features])\n",
    "\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 logistische Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell definieren und trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression für binäre Klassifikation\n",
    "\n",
    "# Pipeline mit Vorverarbeitung und logistische Regression\n",
    "model = LogisticRegression()\n",
    "# Modell trainieren\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen treffen\n",
    "y_pred_log_reg = model.predict(X_test)\n",
    "\n",
    "# Evaluierung\n",
    "accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "classification_rep = classification_report(y_test, y_pred_log_reg)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 klassische Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = DecisionTreeClassifier(random_state=42)\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_tree = clf_tree.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "classification_rep_tree = classification_report(y_test, y_pred_tree)\n",
    "print(f\"Modellgenauigkeit: {accuracy_tree:.2f}\")\n",
    "print(classification_rep_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Bagging in Form von Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Modell trainieren\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_random_forest = clf.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_random_forest)\n",
    "classification_rep = classification_report(y_test, y_pred_random_forest)\n",
    "print(f\"Modellgenauigkeit: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Boosting in Form von AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "adaboost_model = AdaBoostClassifier(\n",
    "    estimator=base_estimator,\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_ada = adaboost_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_random_forest = accuracy_score(y_test, y_pred_ada)\n",
    "classification_rep = classification_report(y_test, y_pred_ada)\n",
    "print(f\"Modellgenauigkeit: {accuracy_random_forest:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basismodelle: KNN, SVM und Logistische Regression\n",
    "base_estimators = [\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),  # KNN mit 5 Nachbarn\n",
    "    ('svc', SVC(kernel='linear', random_state=42)),  # SVM mit linearem Kernel\n",
    "    ('logreg', LogisticRegression(random_state=42))  # Logistische Regression\n",
    "]\n",
    "\n",
    "# Finales Modell (Meta-Modell)\n",
    "final_estimator = LogisticRegression()\n",
    "\n",
    "# StackingClassifier erstellen\n",
    "stacking_model = StackingClassifier(estimators=base_estimators, final_estimator=final_estimator)\n",
    "\n",
    "# Modell trainieren\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "classification_rep = classification_report(y_test, y_pred_stack)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_stack:.2f}\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 k-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 k-Nearest-Neighbor mit euklidischer Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='euclidean')\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 k-Nearest-Neighbor mit manhattan Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='manhattan')\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 k-Nearest-Neighbor mit Minkowski Metrik und p = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10, metric='minkowski', p=3)\n",
    "\n",
    "# Modell trainieren\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "classification_rep_knn = classification_report(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_knn:.2f}\")\n",
    "print(classification_rep_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-NN Modell mit k=10 erstellen\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Modell trainieren\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_rep_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"Modellgenauigkeit: {accuracy_svm:.2f}\")\n",
    "print(classification_rep_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Modell kompilieren\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Modellübersicht anzeigen\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model = create_model(optimizer='sgd')\n",
    "# Modell trainieren\n",
    "history_sgd = sgd_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss_sgd, test_accuracy_sgd = sgd_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Trainingsverlauf visualisieren\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_sgd.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_sgd.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_sgd.history['loss'], label='Training Loss')\n",
    "plt.plot(history_sgd.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred_sgd = sgd_model.predict(X_test)\n",
    "y_pred_classes_sgd = (y_pred_sgd > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix und Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_model = create_model(optimizer='adam')\n",
    "# Modell trainieren\n",
    "history_adam = adam_model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Modell evaluieren\n",
    "test_loss, test_accuracy_adam = adam_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Trainingsverlauf visualisieren\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_adam.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_adam.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_adam.history['loss'], label='Training Loss')\n",
    "plt.plot(history_adam.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Vorhersagen machen\n",
    "y_pred_adam = adam_model.predict(X_test)\n",
    "y_pred_classes_adam = (y_pred_adam > 0.5).astype(int)\n",
    "\n",
    "# Confusion Matrix und Classification Report\n",
    "print(f\"\\nTest Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes_adam))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
